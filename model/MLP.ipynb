{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary for category classes\n",
    "output_dict = {0 : 'aloevera',\n",
    "                1 : 'banana',\n",
    "                2 : 'bilimbi',\n",
    "                3 : 'cantaloupe',\n",
    "                4 : 'cassava',\n",
    "                5 : 'coconut',\n",
    "                6 : 'corn',\n",
    "                7 : 'cucumber',\n",
    "                8 : 'curcuma',\n",
    "                9 : 'eggplant',\n",
    "                10 : 'galangal',\n",
    "                11 : 'ginger',\n",
    "                12 : 'guava',\n",
    "                13 : 'kale',\n",
    "                14 : 'longbeans',\n",
    "                15 : 'mango',\n",
    "                16 : 'melon',\n",
    "                17 : 'orange',\n",
    "                18 : 'paddy',\n",
    "                19 : 'papaya',\n",
    "                20 : 'peperchili',\n",
    "                21 : 'pineapple',\n",
    "                22 : 'pomelo',\n",
    "                23 : 'shallot',\n",
    "                24 : 'soybeans',\n",
    "                25 : 'spinach',\n",
    "                26 : 'sweetpotatoes',\n",
    "                27 : 'tobacco',\n",
    "                28 : 'waterapple',\n",
    "                29 : 'watermelon'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_values(category_num):\n",
    "    return output_dict.get(category_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess images from file paths\n",
    "def load_images(df):\n",
    "    images = []\n",
    "    for file_path in df['image:FILE']:\n",
    "        # Read the image from file\n",
    "        image = cv2.imread(file_path)\n",
    "\n",
    "        # Resize the image\n",
    "        image = cv2.resize(image, (32, 32))  # Adjust size as needed\n",
    "\n",
    "        # Flatten the image to a 1D array\n",
    "        flattened_image = image.flatten()\n",
    "\n",
    "        # Append the flattened image to the list\n",
    "        images.append(flattened_image)\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(file_path):\n",
    "    # Read the csv file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Trim dataset for faster testing for now\n",
    "    #df = df.sample(n=1000, random_state=42)\n",
    "\n",
    "    # Add new column for name of class category\n",
    "    df['label'] = df['category'].apply(map_values)\n",
    "\n",
    "    # Append data/ in front of every image file path\n",
    "    df['image:FILE'] = 'data/' + df['image:FILE']\n",
    "    \n",
    "    # Load images\n",
    "    X = load_images(df)\n",
    "\n",
    "    # Convert images to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(df['label'])\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = prepare_data('data/test.csv')\n",
    "X_val, y_val = prepare_data('data/val.csv')\n",
    "X_test, y_test = prepare_data('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_parameters():\n",
    "    model = MLPClassifier(max_iter=100)\n",
    "\n",
    "    param_grid = {'alpha': [0.01],\n",
    "                  'hidden_layer_sizes': [128]}\n",
    "\n",
    "    randomized_search = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=6, cv=10, random_state=42)\n",
    "    randomized_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = randomized_search.best_params_\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "\n",
    "    # Get the best MLP model from the grid search\n",
    "    best_model = randomized_search.best_estimator_\n",
    "\n",
    "    val_predictions = best_model.predict(X_val)\n",
    "    val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "    print(\"Validation Accuracy:\", val_accuracy)\n",
    "\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_train(X_train, y_train, X_test, y_test):\n",
    "    best_params = finetune_parameters()\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train) \n",
    "    X_train = scaler.transform(X_train)  \n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    best_model = MLPClassifier(**best_params)    \n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    val_accuracy = cross_val_score(best_model, X_train, y_train, cv=10, scoring=\"accuracy\")\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    # Get test accuracy\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Get confusion matrix\n",
    "    confusion_mtrx = confusion_matrix(y_test, y_pred, normalize=\"true\")\n",
    "\n",
    "    return val_accuracy, test_accuracy, confusion_mtrx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_clf_results(val_accuracy, test_accuracy, confusion_mtrx, dataset_name):\n",
    "    print(f\"Performance of Multilayer Perceptron Classification on {dataset_name}:\")\n",
    "    print(\"Cross Validation Accuracy = \", val_accuracy)\n",
    "    print(\"Test Accuracy = \", test_accuracy)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_mtrx.round(decimals=3)) # Round to 3 decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justingalin/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 3 is smaller than n_iter=6. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justingalin/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/justingalin/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/justingalin/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/justingalin/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/justingalin/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/justingalin/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'hidden_layer_sizes': 128, 'alpha': 0.01}\n",
      "Validation Accuracy: 0.03333333333333333\n",
      "Performance of Multilayer Perceptron Classification on Final Dataset:\n",
      "Cross Validation Accuracy =  0.5598333333333333\n",
      "Test Accuracy =  0.9696666666666667\n",
      "Confusion Matrix:\n",
      "[[1.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    1.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.49  0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.51  0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    1.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    1.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    1.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    1.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    1.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.\n",
      "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.005 0.995\n",
      "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  1.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    1.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    1.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    1.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.395 0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.605 0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    1.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.    1.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.    0.    1.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.    0.    0.    1.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.\n",
      "  0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.\n",
      "  0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  1.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    1.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    1.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    1.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    1.   ]]\n"
     ]
    }
   ],
   "source": [
    "val_acc_final, test_acc_final, confusion_mtrx_final = MLP_train(X_train, y_train, X_test, y_test)\n",
    "print_clf_results(val_acc_final, test_acc_final, confusion_mtrx_final, \"Final Dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
